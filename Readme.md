ğŸ“˜ Projeto FX USD/BRL â€“ Data Lakehouse
ğŸ“‚ Estrutura de Notebooks

Este repositÃ³rio contÃ©m trÃªs notebooks principais que implementam o fluxo Bronze â†’ Silver â†’ Gold seguindo a arquitetura de camadas Delta Lake:

01_ingest_fx_usdbrl_bronze.ipynb

Camada Bronze (Raw)

Objetivo: ingestÃ£o de dados de cÃ¢mbio (USD/BRL) a partir de API externa.

CaracterÃ­sticas:

Estrutura append-only (dados imutÃ¡veis, sem updates/deletes).

Armazenamento em Delta Table.

ConexÃ£o com Azure Data Lake Storage (ADLS) via SAS Token ou Secret Scope.

02_transform_fx_usdbrl_silver.ipynb

Camada Silver (Curated/Enriched)

Objetivo: padronizar, limpar e enriquecer os dados de cÃ¢mbio.

Principais transformaÃ§Ãµes:

ConversÃ£o de tipos.

NormalizaÃ§Ã£o de schema.

DeduplicaÃ§Ã£o de registros.

Estrutura de merge incremental (upsert) com Delta Lake.

03_gold_fx_usdbrl_dashboard.ipynb

Camada Gold (Analytics/BI)

Objetivo: disponibilizar dataset pronto para consumo em dashboards.

CaracterÃ­sticas:

Dataset final agregado por dia.

Preparado para ser consumido em ferramentas de BI (Power BI, Fabric Lakehouse, etc).

ğŸ”„ Fluxo de Dados
flowchart LR
    A[API FX Provider] -->|IngestÃ£o| B[Bronze]
    B -->|Limpeza/TransformaÃ§Ã£o| C[Silver]
    C -->|AgregaÃ§Ã£o/Curadoria| D[Gold]
    D -->|Consumo| E[Dashboard / Power BI]

âš™ï¸ ExecuÃ§Ã£o

Configure os segredos:

Usando Databricks Secret Scope ou Azure Key Vault para armazenar tokens de acesso (recomendado).

Exemplo de uso no notebook:

sas = dbutils.secrets.get(scope="storage-secrets", key="sas-token")


Execute os notebooks na ordem:

01_ingest_fx_usdbrl_bronze.ipynb

02_transform_fx_usdbrl_silver.ipynb

03_gold_fx_usdbrl_dashboard.ipynb

Verifique no Data Explorer / OneLake / ADLS se as tabelas Delta foram criadas corretamente.

ğŸ“Š Estrutura de Tabelas

Bronze: fx_usdbrl_raw (dados brutos, append-only).

Silver: fx_usdbrl_clean (dados deduplicados e normalizados).

Gold: fx_usdbrl_dashboard (dataset final pronto para anÃ¡lise).

ğŸš€ PrÃ³ximos Passos

Automatizar execuÃ§Ã£o via Azure Data Factory ou Fabric Pipelines.

Implementar testes de qualidade de dados (DQ checks).

Configurar alertas e monitoramento em caso de falha de ingestÃ£o.
